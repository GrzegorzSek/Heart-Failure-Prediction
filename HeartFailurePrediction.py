# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10_ihnwIc7aA6uI3G_Vc7ZXHaXgqELj0Y

---


<center><h1>ALgorytmy wspomagania decyzji</h1></center>
<center><h2>Temat: Przewidywanie niewydolności serca</h2></center>
<center><h2>Prowadzący: dr inż. Piotr Ciskowski</h2></center>
<center><h2>Autorzy: Grzegorz Sęk 241778, Hubert Ozga 235414</h2></center>
<center><h2>Ocena: 5.0</h2></center>

---



Wykorzystany dataset: [Heart Failure Prediction](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data)

Dodatkowe informacje:


*   SEX -- male = 1, female = 0

*   Diabetes, Anaemia, High_blood_preassure, smoking, Death_Event -- 0 = NO, 1 = YES


tłumaczenie kolumn:
* wiek
* anemia (niedokrwistość)
* Kinaza kreatynowa (to enzym zaangażowany w przemiany energetyczne organizmu)
* cukrzyca
* Frakcja wyrzutowa (stosunek objętości wyrzutowej serca do objętości           końcoworozkurczowej komory serca)
* wysokie ciśnienie
* płytki krwi
* kreatynina w surowicy
* sód w surowicy
* płeć
* palenie papierosów
* czas
* śmierć

---
<center><h3>Opis programu:</h3></center>

Confusion matrix:
P - prawidłowo
N - nieprawidłowo

 0P 1N

 0N 1P

<center><h3>Wykorzystane modele:</h3></center>

1.   Random Forest Classifier: 

[Dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)


[Opis działania](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)

[Opis działania (YT)](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&ab_channel=StatQuestwithJoshStarmer)

2.   SVM 

[Dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)


[Opis działania](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)

3.   Sieci neuronowe 

[sklearn](https://scikit-learn.org/stable/glossary.html#term-warm_start)

[Dokumentacja 1](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)

[Dokumentacja 2](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)


*   SGD
[Opis działania](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)
*   ADAM
[Opis działania](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
*   Limited-memory BFGS
[Opis działania](https://towardsdatascience.com/limited-memory-broyden-fletcher-goldfarb-shanno-algorithm-in-ml-net-118dec066ba)
---
"""

# Commented out IPython magic to ensure Python compatibility.
# ------------------------------------------------- importy
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier 
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
#
from sklearn import metrics
import warnings
import numpy as np
from sklearn import svm
import sklearn
warnings.filterwarnings('ignore')
# %matplotlib inline
accuracy_dead = [] # zbiera skuteczność ze wszystkich modeli 
accuracy_alive = [] # zbiera skuteczność ze wszystkich modeli

# ------------------------------------------------- wczytywanie i prezentacja danych
data = pd.read_csv('heart_failure_clinical_records_dataset.csv', sep = ',')
data.head() # wyświetla tabelę z danymi
# data.info() # informacje na temat danych
# data.isnull().sum() # inforamcje o pustych komórkach
data['DEATH_EVENT'].value_counts() # zlicza żywych i martwych
sns.countplot(data['DEATH_EVENT']) # wykres kolumnowy (żywi, martwi)

# ------------------------------------------------- przygotowanie danych
# oddzielenie response variable od feature variable
X = data.drop('DEATH_EVENT', axis = 1)
y = data['DEATH_EVENT']

# podział na zestaw trenujący i zestaw testowy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) # random_state -- random seed number (losowo przypisuje dane jako testowe i treningowe)
# print(y_train)
# Skalowanie danych -- wiekszość modeli tego wymaga
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# ------------------------------------- MODEl 1 ------------------------------------
# random forest classifier
RFC_result = []
rfc = RandomForestClassifier(n_estimators = 1000) 
rfc.fit(X_train, y_train) 
pred_rfc = rfc.predict(X_test)

# efekty modelu
RFC_result=classification_report(y_test, pred_rfc, output_dict=True)

accuracy_alive.append(RFC_result['0']['precision'])
accuracy_dead.append(RFC_result['1']['precision'])

print(classification_report(y_test, pred_rfc))
print(confusion_matrix(y_test, pred_rfc))

# ------------------------------------- MODEl 2 ------------------------------------
# SVM Classifier
SVM_result = []
clf = svm.SVC()
clf.fit(X_train, y_train)
pred_clf = clf.predict(X_test)

# efekty modelu
SVM_result=classification_report(y_test, pred_clf, output_dict=True)
accuracy_alive.append(SVM_result['0']['precision'])
accuracy_dead.append(SVM_result['1']['precision'])

print(classification_report(y_test, pred_clf))
print(confusion_matrix(y_test, pred_clf))

# ------------------------------------- MODEl 3 -- SIECI NEURONOWE ------------------------------------
# z podziałem na iteracje
# mlpc = MLPClassifier(hidden_layer_sizes = (11,11,11), max_iter = 5000)
mlpc = MLPClassifier(solver='adam', hidden_layer_sizes = (11), max_iter=1, warm_start=True)
# mlpc = MLPClassifier(solver='sgd', hidden_layer_sizes = (11), max_iter=1, warm_start=True)
# mlpc = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15, 15), random_state=1)
epochs = 35
epochs_counter = []
epochs_results_alive = []
epochs_results_dead = []
for epoch in range(0, epochs):
  # pobranie danych i ustawienie
  data = pd.read_csv('heart_failure_clinical_records_dataset.csv', sep = ',')
  # oddzielenie response variable od feature variable
  X = data.drop('DEATH_EVENT', axis = 1)
  y = data['DEATH_EVENT']
  # podział na zestaw trenujący i zestaw testowy
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) # random_state -- random seed number (losowo przypisuje dane jako testowe i treningowe)
  # print(y_train)
  # skalowanie danych
  sc = StandardScaler()
  X_train = sc.fit_transform(X_train)
  X_test = sc.fit_transform(X_test)
  #----------
  epochs_counter.append(epoch)
  number_of_rows = X_train.shape[0]
  # print(X_train.shape[0])
  number_of_train_rows = 240
  idx = 0
  counter = 1;
  divider = 1
  zywi = []
  martwi = []
  x = []
  wynik_dict = []
  while(divider >= 1):
    divider = (number_of_rows - idx) / number_of_train_rows
    # print("divider: ", divider)
    if(divider == 0):
      break
    X = X_train[idx : idx + number_of_train_rows]
    Y = y_train[idx : idx + number_of_train_rows]
    # print(X.shape)
    idx = idx + number_of_train_rows
    x.append(counter)
    mlpc.fit(X, Y)
    # mlpc.partial_fit(X,Y, classes=[0, 1]) 
    pred_mlpc = mlpc.predict(X_test)
    # efekty
    wynik_dict = classification_report(y_test, pred_mlpc, output_dict=True)
    zywi.append(wynik_dict['0']['precision'])
    martwi.append(wynik_dict['1']['precision'])
    # print(confusion_matrix(y_test, pred_mlpc))
    counter = counter + 1
  # plt.plot(x, zywi, 'red')
  # plt.plot(x, martwi, 'green')
  # plt.legend(['alive', 'dead'])
  # plt.xlabel('iteration')
  # plt.ylabel('accuracy')
  # plt.title('iterations comparison (epoch: ' + str(epoch + 1) + ')')
  # plt.show()
  epochs_results_alive.append(wynik_dict['0']['precision'])
  epochs_results_dead.append(wynik_dict['1']['precision'])

accuracy_alive.append(epochs_results_alive[-1])
accuracy_dead.append(epochs_results_dead[-1])

print(epochs_results_alive[-1])
print(epochs_results_dead[-1])

plt.hist(y_train, color ='red')
plt.hist(y_test, color ='green')
plt.show()

# epoki
plt.plot(epochs_counter, epochs_results_alive, 'red')
plt.plot(epochs_counter, epochs_results_dead, 'green')
plt.legend(['alive', 'dead'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.title('epochs comparison')
plt.show()

conf_mat = confusion_matrix(y_test, pred_mlpc)
# print(conf_mat)
objects = ['0P', '0N', '1P', '1N']
y_pos = np.arange(len(objects))
performance = [conf_mat[0][0], conf_mat[1][0], conf_mat[1][1], conf_mat[0][1]]
plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)

# Porównanie wszystkich modeli
n_groups = 3

# create plot
fig, ax = plt.subplots()
fig.set_size_inches(18.5, 10.5)
index = np.arange(n_groups)
bar_width = 0.35
opacity = 0.8

rects1 = plt.bar(index, accuracy_alive, bar_width, alpha=opacity, color='b', label='Alive')

rects2 = plt.bar(index + bar_width, accuracy_dead, bar_width, alpha=opacity, color='g', label='Dead')

plt.xlabel('model')
plt.ylabel('accuracy')
plt.title('Model comparison')
plt.xticks(index + bar_width, ('Random Forest', 'SVM', 'NN'))
plt.legend()

# wartosci nad kolumnami
# assign your bars to a variable so their attributes can be accessed
# bars = plt.bar(x, height=y, width=.4)

# # access the bar attributes to place the text in the appropriate location
# for bar in bars:
#     yval = bar.get_height()
#     plt.text(bar.get_x(), yval + .005, yval)
# -------------

plt.tight_layout()
plt.show()

# mlpc = MLPClassifier(solver='adam', hidden_layer_sizes = (10,10,10), max_iter=1, warm_start=True) #, batch_size=80)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) # random_state -- random seed number (losowo przypisuje dane jako testowe i treningowe)
# # print(y_train)
# # skalowanie danych
# sc = StandardScaler()
# X_train = sc.fit_transform(X_train)
# X_test = sc.fit_transform(X_test)
# for i in range(1, 10):
#   mlpc.fit(X_train, y_train)
#   # mlpc.partial_fit(X,Y, classes=[0, 1]) 
#   pred_mlpc = mlpc.predict(X_test)
#   # efekty
#   print(classification_report(y_test, pred_mlpc))
